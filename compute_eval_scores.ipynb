{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation script imported from https://github.com/gsarti/it5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "fst = load_dataset(\"it5/datasets\", \"fst\")\n",
    "hg = load_dataset(\"it5/datasets\", \"hg\")\n",
    "ns = load_dataset(\"it5/datasets\", \"ns\")\n",
    "qa = load_dataset(\"it5/datasets\", \"qa\")\n",
    "qg = load_dataset(\"it5/datasets\", \"qg\")\n",
    "st_g2r = load_dataset(\"it5/datasets\", \"st_g2r\")\n",
    "st_r2g = load_dataset(\"it5/datasets\", \"st_r2g\")\n",
    "wits = load_dataset(\"it5/datasets\", \"wits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"references/informal.txt\", 'w') as f:\n",
    "    for s in fst[\"test_0\"][\"informal\"]:\n",
    "        f.write(s.lower() + '\\n')\n",
    "\n",
    "for i in range(4):\n",
    "    with open(f\"references/formal{i}.txt\", 'w') as f:\n",
    "        for s in fst[f\"test_{i}\"][\"formal\"]:\n",
    "            f.write(s.lower() + '\\n')\n",
    "\n",
    "with open(f\"references/hg.txt\", 'w') as f:\n",
    "    for s in hg[f\"test\"][\"target\"]:\n",
    "        f.write(s.lower() + '\\n')\n",
    "\n",
    "for name in [\"fanpage\", \"ilpost\"]:\n",
    "    with open(f\"references/{name}.txt\", 'w') as f:\n",
    "        for s in ns[f\"test_{name}\"][\"target\"]:\n",
    "            f.write(s.replace(\"\\n\", \"\").lower() + '\\n')\n",
    "\n",
    "with open(f\"references/qa.txt\", 'w') as f:\n",
    "    for s in qa[f\"test\"][\"target\"]:\n",
    "        f.write(s.lower() + '\\n')\n",
    "\n",
    "with open(f\"references/qg.txt\", 'w') as f:\n",
    "    for s in qg[f\"test\"][\"target\"]:\n",
    "        f.write(s.lower() + '\\n')\n",
    "\n",
    "with open(f\"references/st_g2r.txt\", 'w') as f:\n",
    "    for s in st_g2r[f\"test\"][\"headline\"]:\n",
    "        f.write(s.lower() + '\\n')\n",
    "\n",
    "with open(f\"references/st_r2g.txt\", 'w') as f:\n",
    "    for s in st_r2g[f\"test\"][\"headline\"]:\n",
    "        f.write(s.lower() + '\\n')\n",
    "\n",
    "with open(f\"references/wits.txt\", 'w') as f:\n",
    "    for s in wits[f\"test\"][\"summary\"]:\n",
    "        f.write(s.lower() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93956/3536591677.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "rouge = load_metric(\"rouge\")\n",
    "bertscore = load_metric(\"bertscore\")\n",
    "\n",
    "rouge_kwargs = {\n",
    "    'rouge_types': [\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "    'use_aggregator': False,\n",
    "    'use_stemmer': False,\n",
    "}\n",
    "bertscore_kwargs = {\n",
    "    \"model_type\": \"configs/bert-base-italian-xxl-uncased\",\n",
    "    \"lang\": \"it\",\n",
    "    \"num_layers\": 10,\n",
    "    \"batch_size\": 16,\n",
    "    \"rescale_with_baseline\": True,\n",
    "    \"baseline_path\": \"configs/bertscore_baseline_ita.tsv\",\n",
    "    \"use_fast_tokenizer\": True\n",
    "}\n",
    "\n",
    "result_path = \"eval/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install absl-py nltk rouge_score bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fst_i2f\n",
      "camoscio-7b {'rouge1': 0.6220582572775898, 'rouge2': 0.4285147347040223, 'rougeL': 0.6006278042777528}\n"
     ]
    }
   ],
   "source": [
    "result_path = \"eval/results\"\n",
    "\n",
    "def compute_i2f(model, metric, kwargs):\n",
    "    with open(\"references/formal0.txt\", 'r') as f:\n",
    "        ref0 = [s.strip() for s in f.readlines()]\n",
    "    with open(\"references/formal1.txt\", 'r') as f:\n",
    "        ref1 = [s.strip() for s in f.readlines()]\n",
    "    with open(\"references/formal2.txt\", 'r') as f:\n",
    "        ref2 = [s.strip() for s in f.readlines()]\n",
    "    with open(\"references/formal3.txt\", 'r') as f:\n",
    "        ref3 = [s.strip() for s in f.readlines()]\n",
    "    with open(f\"{result_path}/{model}-informal-to-formal{'-ita' if 'mt5' in model else ''}_test_0.txt\", 'r') as f:\n",
    "        hyp = [s.strip() for s in f.readlines()]\n",
    "    scores0 = {k:[s.fmeasure for s in v] for k, v in metric.compute(predictions=hyp, references=ref0, **kwargs).items()}\n",
    "    scores1 = {k:[s.fmeasure for s in v] for k, v in metric.compute(predictions=hyp, references=ref1, **kwargs).items()}\n",
    "    scores2 = {k:[s.fmeasure for s in v] for k, v in metric.compute(predictions=hyp, references=ref2, **kwargs).items()}\n",
    "    scores3 = {k:[s.fmeasure for s in v] for k, v in metric.compute(predictions=hyp, references=ref3, **kwargs).items()}\n",
    "    return {\n",
    "        s:sum([max(x0, x1, x2, x3) for x0, x1, x2, x3 in zip(scores0[s], scores1[s], scores2[s], scores3[s])])/len(scores0[s])\n",
    "        for s in scores0.keys()\n",
    "    }\n",
    "\n",
    "print(\"fst_i2f\")\n",
    "print(\"camoscio-7b\", compute_i2f(\"camoscio-7b\", rouge, rouge_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fst_f2i\n",
      "camoscio-7b {'rouge1': 0.6450322843198721, 'rouge2': 0.4365721944806926, 'rougeL': 0.6233499493070205}\n"
     ]
    }
   ],
   "source": [
    "result_path = \"eval/results\"\n",
    "\n",
    "def compute_f2i(model, metric, kwargs):\n",
    "    with open(\"references/informal.txt\", 'r') as f:\n",
    "        ref = [s.strip() for s in f.readlines()]\n",
    "    with open(f\"{result_path}/{model}-formal-to-informal{'-ita' if 'mt5' in model else ''}_test_0.txt\", 'r') as f:\n",
    "        hyp0 = [s.strip() for s in f.readlines()]\n",
    "    with open(f\"{result_path}/{model}-formal-to-informal{'-ita' if 'mt5' in model else ''}_test_1.txt\", 'r') as f:\n",
    "        hyp1 = [s.strip() for s in f.readlines()]\n",
    "    with open(f\"{result_path}/{model}-formal-to-informal{'-ita' if 'mt5' in model else ''}_test_2.txt\", 'r') as f:\n",
    "        hyp2 = [s.strip() for s in f.readlines()]\n",
    "    with open(f\"{result_path}/{model}-formal-to-informal{'-ita' if 'mt5' in model else ''}_test_3.txt\", 'r') as f:\n",
    "        hyp3 = [s.strip() for s in f.readlines()]\n",
    "    scores0 = {k:[s.fmeasure for s in v] for k, v in metric.compute(predictions=hyp0, references=ref, **kwargs).items()}\n",
    "    scores1 = {k:[s.fmeasure for s in v] for k, v in metric.compute(predictions=hyp1, references=ref, **kwargs).items()}\n",
    "    scores2 = {k:[s.fmeasure for s in v] for k, v in metric.compute(predictions=hyp2, references=ref, **kwargs).items()}\n",
    "    scores3 = {k:[s.fmeasure for s in v] for k, v in metric.compute(predictions=hyp3, references=ref, **kwargs).items()}\n",
    "    return {\n",
    "        s:sum([max(x0, x1, x2, x3) for x0, x1, x2, x3 in zip(scores0[s], scores1[s], scores2[s], scores3[s])])/len(scores0[s])\n",
    "        for s in scores0.keys()\n",
    "    }\n",
    "\n",
    "print(\"fst_f2i\")\n",
    "print(\"camoscio-7b\", compute_f2i(\"camoscio-7b\", rouge, rouge_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bertscore f2i\n",
      "camoscio-7b 0.6518663733750582\n"
     ]
    }
   ],
   "source": [
    "result_path = \"eval/results\"\n",
    "\n",
    "def compute_f2i(model, metric, kwargs):\n",
    "    with open(\"references/informal.txt\", 'r') as f:\n",
    "        ref = [s.strip() for s in f.readlines()]\n",
    "    with open(f\"{result_path}/{model}-formal-to-informal{'-ita' if 'mt5' in model else ''}_test_0.txt\", 'r') as f:\n",
    "        hyp0 = [s.strip() for s in f.readlines()]\n",
    "    with open(f\"{result_path}/{model}-formal-to-informal{'-ita' if 'mt5' in model else ''}_test_1.txt\", 'r') as f:\n",
    "        hyp1 = [s.strip() for s in f.readlines()]\n",
    "    with open(f\"{result_path}/{model}-formal-to-informal{'-ita' if 'mt5' in model else ''}_test_2.txt\", 'r') as f:\n",
    "        hyp2 = [s.strip() for s in f.readlines()]\n",
    "    with open(f\"{result_path}/{model}-formal-to-informal{'-ita' if 'mt5' in model else ''}_test_3.txt\", 'r') as f:\n",
    "        hyp3 = [s.strip() for s in f.readlines()]\n",
    "    scores0 = metric.compute(predictions=hyp0, references=ref, **kwargs)[\"f1\"]\n",
    "    scores1 = metric.compute(predictions=hyp1, references=ref, **kwargs)[\"f1\"]\n",
    "    scores2 = metric.compute(predictions=hyp2, references=ref, **kwargs)[\"f1\"]\n",
    "    scores3 = metric.compute(predictions=hyp3, references=ref, **kwargs)[\"f1\"]\n",
    "    return sum([max(x0, x1, x2, x3) for x0, x1, x2, x3 in zip(scores0, scores1, scores2, scores3)])/len(scores0)\n",
    "\n",
    "print(\"bertscore f2i\")\n",
    "print(\"camoscio-7b\", compute_f2i(\"camoscio-7b\", bertscore, bertscore_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rouge_kwargs = {\n",
    "    'rouge_types': [\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "}\n",
    "\n",
    "def compute_rouge(ref_file, hyp_file, kwargs):\n",
    "    with open(f\"references/{ref_file}.txt\", 'r') as f:\n",
    "        ref = [s.strip() for s in f.readlines()]\n",
    "    with open(f\"{result_path}/{hyp_file}.txt\", 'r') as f:\n",
    "        hyp = [s.strip() for s in f.readlines()]\n",
    "    return {k:v.mid.fmeasure for k,v in rouge.compute(predictions=hyp, references=ref, **kwargs).items()}\n",
    "\n",
    "\n",
    "print(\"ns_ilpost\")\n",
    "print(\"camoscio-7b\", compute_rouge(\"ilpost\", \"llama-7b-hf_news-summarization_test_ilpost\", rouge_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ns_fanpage\n",
      "camoscio-7b {'rouge1': 0.2504043047929348, 'rouge2': 0.10648184368593602, 'rougeL': 0.1715946228680273}\n"
     ]
    }
   ],
   "source": [
    "print(\"ns_fanpage\")\n",
    "print(\"camoscio-7b\", compute_rouge(\"fanpage\", \"llama-7b-hf_ns_test_fanpage\", rouge_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bertscore i2f\n",
      "camoscio-7b 0.6672154705971479\n"
     ]
    }
   ],
   "source": [
    "def compute_i2f(model, metric, kwargs):\n",
    "    with open(\"references/formal0.txt\", 'r') as f:\n",
    "        ref0 = [s.strip() for s in f.readlines()]\n",
    "    with open(\"references/formal1.txt\", 'r') as f:\n",
    "        ref1 = [s.strip() for s in f.readlines()]\n",
    "    with open(\"references/formal2.txt\", 'r') as f:\n",
    "        ref2 = [s.strip() for s in f.readlines()]\n",
    "    with open(\"references/formal3.txt\", 'r') as f:\n",
    "        ref3 = [s.strip() for s in f.readlines()]\n",
    "    with open(f\"{result_path}/{model}-informal-to-formal{'-ita' if 'mt5' in model else ''}_test_0.txt\", 'r') as f:\n",
    "        hyp = [s.strip() for s in f.readlines()]\n",
    "\n",
    "    scores0 = metric.compute(predictions=hyp, references=ref0, **kwargs)[\"f1\"]\n",
    "    scores1 = metric.compute(predictions=hyp, references=ref1, **kwargs)[\"f1\"]\n",
    "    scores2 = metric.compute(predictions=hyp, references=ref2, **kwargs)[\"f1\"]\n",
    "    scores3 = metric.compute(predictions=hyp, references=ref3, **kwargs)[\"f1\"]\n",
    "    return sum([max(x0, x1, x2, x3) for x0, x1, x2, x3 in zip(scores0, scores1, scores2, scores3)])/len(scores0)\n",
    "\n",
    "print(\"bertscore i2f\")\n",
    "print(\"camoscio-7b\", compute_i2f(\"camoscio-7b\", bertscore, bertscore_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bertscore(ref_file, hyp_file, kwargs):\n",
    "    with open(f\"references/{ref_file}.txt\", 'r') as f:\n",
    "        ref = [s.strip() for s in f.readlines()]\n",
    "    with open(f\"{result_path}/{hyp_file}.txt\", 'r') as f:\n",
    "        hyp = [s.strip() for s in f.readlines()]\n",
    "    scores = bertscore.compute(predictions=hyp, references=ref, **kwargs)[\"f1\"]\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ns_ilpost\")\n",
    "print(\"camoscio-7b\", compute_bertscore(\"ilpost\", \"llama-7b-hf_news-summarization_test_ilpost\", bertscore_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"ns_fanpage\")\n",
    "print(\"camoscio-7b\", compute_bertscore(\"fanpage\", \"llama-7b-hf_ns_test_fanpage\", bertscore_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQUAD eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset squad_it (/home/andrea/.cache/huggingface/datasets/squad_it/default/0.1.0/d442bdb4794b4bae227ab19105b76d706ed7cf2ac342e4c9da4a5c36bde19d71)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fa236686ed4c328399d597b789069e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': '5725b33f6a3fe71400b8952d',\n",
       " 'context': 'La crisi petrolifera del 1973 iniziÃ² nell\\' ottobre 1973 quando i membri dell\\' Organizzazione dei Paesi esportatori di petrolio arabo (OAPEC, composta dai membri arabi dell\\' OPEC piÃ¹ Egitto e Siria) proclamarono un embargo petrolifero. Alla fine dell\\' embargo, nel marzo 1974, il prezzo del petrolio era salito da 3 dollari al barile a quasi 12 dollari a livello mondiale; i prezzi americani erano notevolmente piÃ¹ elevati. L\\' embargo ha causato una crisi petrolifera, o \"shock\", con molti effetti a breve e lungo termine sulla politica globale e sull\\' economia globale. PiÃ¹ tardi fu chiamato il \"primo shock petrolifero\", seguito dalla crisi petrolifera del 1979, definita il \"secondo shock petrolifero\".',\n",
       " 'question': 'Quando Ã¨ iniziata la crisi petrolifera del 1973?',\n",
       " 'answers': {'text': ['ottobre 1973',\n",
       "   'ottobre 1973',\n",
       "   'ottobre 1973',\n",
       "   'ottobre',\n",
       "   '1973'],\n",
       "  'answer_start': [43, 43, 43, 43, 25]}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import sys\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "\n",
    "squad_it = load_dataset(\"squad_it\")\n",
    "squad_it[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(txt):\n",
    "  def remove_articles(text):\n",
    "      return re.sub(r'\\b(il|lo|la|i|gli|le|l)\\b', ' ', text)\n",
    "  def remove_prepositions(text):\n",
    "      return re.sub(r'\\b(di|a|da|in|con|su|per|tra|fra)\\b', ' ', text)\n",
    "  def white_space_fix(text):\n",
    "      return ' '.join(text.split())\n",
    "  def remove_punc(text):\n",
    "      exclude = set(string.punctuation)\n",
    "      return ''.join(ch for ch in text if ch not in exclude)\n",
    "  def lower(text):\n",
    "      return text.lower()\n",
    "  return white_space_fix(\n",
    "      remove_punc(\n",
    "          remove_prepositions(remove_articles(lower(txt)))\n",
    "      )\n",
    "  )\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    return max([metric_fn(prediction, ground_truth) for ground_truth in ground_truths])\n",
    "\n",
    "\n",
    "def evaluate(predictions):\n",
    "    f1 = []\n",
    "    exact_match = []\n",
    "    for ref, prediction in zip(squad_it[\"test\"], predictions):\n",
    "        exact_match.append(metric_max_over_ground_truths(exact_match_score, prediction, ref[\"answers\"][\"text\"]))\n",
    "        f1.append(metric_max_over_ground_truths(f1_score, prediction, ref[\"answers\"][\"text\"]))\n",
    "    exact_match = sum(exact_match) / len(exact_match)\n",
    "    f1 = sum(f1) / len(f1)\n",
    "\n",
    "    return {\"exact_match\": exact_match, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camoscio-7b {'exact_match': 0.07701406229465108, 'f1': 0.27015108152913614}\n"
     ]
    }
   ],
   "source": [
    "print(\"camoscio-7b\", evaluate(open(f'{result_path}/llama-7b-hf_question-answering_test.txt', 'r').read().splitlines()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "camoscio-7b {'exact_match': 0.07701406229465108, 'f1': 0.27015108152913614}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_bertscore(ref_file, hyp_file, kwargs):\n",
    "    with open(f\"references/{ref_file}.txt\", 'r') as f:\n",
    "        ref = [s.strip() for s in f.readlines()]\n",
    "    with open(f\"{result_path}/{hyp_file}.txt\", 'r') as f:\n",
    "        hyp = [s.strip() for s in f.readlines()]\n",
    "    scores = bertscore.compute(predictions=hyp, references=ref, **kwargs)[\"f1\"]\n",
    "    return sum(scores) / len(scores)\n",
    "    #return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQUAD-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camoscio-7b 0.2373131654580585\n"
     ]
    }
   ],
   "source": [
    "print(\"SQUAD-it\")\n",
    "print(\"camoscio-7b\", compute_bertscore(\"qa\", \"llama-7b-hf_question-answering_test\", bertscore_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camoscio-7b {'rouge1': 0.2429478453736345, 'rouge2': 0.1333382459595141, 'rougeL': 0.2416336943874583}\n"
     ]
    }
   ],
   "source": [
    "print(\"camoscio-7b\", compute_rouge(\"qa\", \"llama-7b-hf_question-answering_test\", rouge_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mt5-small {'rouge1': 0.6176649280125064, 'rouge2': 0.34690279865702467, 'rougeL': 0.6173512514812024}\n",
      "mt5-small 0.712143312844745\n"
     ]
    }
   ],
   "source": [
    "print(\"mt5-small\", compute_rouge(\"qa\", \"mt5-small-question-answering-ita_test\", rouge_kwargs))\n",
    "print(\"mt5-small\", compute_bertscore(\"qa\", \"mt5-small-question-answering-ita_test\", bertscore_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mt5-base {'rouge1': 0.709509904008709, 'rouge2': 0.396262003262724, 'rougeL': 0.7089794174839387}\n",
      "mt5-base 0.7704119623410816\n"
     ]
    }
   ],
   "source": [
    "print(\"mt5-base\", compute_rouge(\"qa\", \"mt5-base-question-answering-ita_test\", rouge_kwargs))\n",
    "print(\"mt5-base\", compute_bertscore(\"qa\", \"mt5-base-question-answering-ita_test\", bertscore_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it5-small {'rouge1': 0.6712948598730601, 'rouge2': 0.3722742816420268, 'rougeL': 0.6713140331352194}\n",
      "it5-small 0.7437164323812749\n"
     ]
    }
   ],
   "source": [
    "print(\"it5-small\", compute_rouge(\"qa\", \"it5-small-question-answering_test\", rouge_kwargs))\n",
    "print(\"it5-small\", compute_bertscore(\"qa\", \"it5-small-question-answering_test\", bertscore_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it5-base {'rouge1': 0.7126444413419158, 'rouge2': 0.40612353674329427, 'rougeL': 0.7124650680793831}\n",
      "it5-base 0.7707937262429813\n"
     ]
    }
   ],
   "source": [
    "print(\"it5-base\", compute_rouge(\"qa\", \"it5-base-question-answering_test\", rouge_kwargs))\n",
    "print(\"it5-base\", compute_bertscore(\"qa\", \"it5-base-question-answering_test\", bertscore_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it5-large {'rouge1': 0.730023661619623, 'rouge2': 0.4120094697587379, 'rougeL': 0.7297545452055703}\n",
      "it5-large 0.7845929185766455\n"
     ]
    }
   ],
   "source": [
    "print(\"it5-large\", compute_rouge(\"qa\", \"it5-large-question-answering_test\", rouge_kwargs))\n",
    "print(\"it5-large\", compute_bertscore(\"qa\", \"it5-large-question-answering_test\", bertscore_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact match via ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('eval/em_gpt_results.json') as f:\n",
    "\tlines = f.readlines()\n",
    "\tnew_list = []\n",
    "\tfor line in lines:\n",
    "\t\tnew_list.append(json.loads(line))\n",
    "\n",
    "\n",
    "models = {\n",
    "\t'it5-base': [],\n",
    "\t'mt5-small': [],\n",
    "\t'llama-7b-hf': [],\n",
    "\t'it5-small': [],\n",
    "\t'mt5-base': [],\n",
    "\t'it5-large': []\n",
    "}\n",
    "\n",
    "def get_elem(element, key):\n",
    "\tif element.get(key):\n",
    "\t\tmodels[key].append(element.get(key))\n",
    "\telif element.get(f'A {key}'):\n",
    "\t\tmodels[key].append(element.get(f'A {key}'))\n",
    "\telse:\n",
    "\t\tmodels[key].append(0)\n",
    "\n",
    "models_list = models.keys()\n",
    "\n",
    "for element in new_list:\n",
    "\tfor model in models_list:\n",
    "\t\tget_elem(element, model)\n",
    "\n",
    "print(\"Accuracy\")\n",
    "for model in models:\n",
    "\tprint(model, sum(models[model])/len(models[model]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85d98b63f393548f3009c8d52d8286e609a1467b1184fe464fb700873fbd3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
